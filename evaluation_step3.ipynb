{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, process biweekly data and save the data into step 3 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the result from data exploring, and processing them through PCA, to descope 10 features into 5\n",
    "# Extract 10 effective features from the original dataset and save them\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def main():\n",
    "    processedList = []\n",
    "\n",
    "    loadPath = 'yourloadpath'\n",
    "    savePath = 'yoursavepath'\n",
    "\n",
    "    fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "#     fileList = ['002645.SZ.csv']\n",
    "    \n",
    "    for f in fileList:\n",
    "        fileName = loadPath + f\n",
    "        data = pd.read_csv(fileName)\n",
    "        idxList = data.index.values.tolist()\n",
    "\n",
    "        # save all column names that should be done normalization into the list below\n",
    "        colName = [\n",
    "                     'ts_code',\n",
    "                     'trade_date', \n",
    "                     'MTM_10_plr',\n",
    "                     'ROC_10_plr',\n",
    "                     'WNR_9',\n",
    "                     'WNR_9_maxmin',\n",
    "                     'SLOWK',\n",
    "                     'SLOWK_maxmin',\n",
    "                     'ROC_10',\n",
    "                     'SLOWD_flc',\n",
    "                     'WNR_9_flc',\n",
    "                     'RSI_5',\n",
    "                     'BIAS_20_maxmin',\n",
    "                     'RSI_5_maxmin',\n",
    "                     'BIAS_20',\n",
    "                     'SMA_10',\n",
    "                     'SLOWD',\n",
    "                     'SMA_10_maxmin',\n",
    "                     'CCI_24',\n",
    "                     'SLOWD_maxmin',\n",
    "                     'SMA_10_flc',\n",
    "                     'pct_chg',\n",
    "                     'MACD_HIST',\n",
    "                     'BR_26',\n",
    "                     'low',\n",
    "                     'change',\n",
    "                     'ADOSC',\n",
    "                     'SLOWK_flc',\n",
    "                     'RSI_5_flc',\n",
    "                     'MTM_10',\n",
    "                     'ADOSC_maxmin',\n",
    "                     'w_up_down'\n",
    "                  ]\n",
    "        try:\n",
    "            data = data[colName]\n",
    "            fileName = savePath + f\n",
    "            data.to_csv(fileName, encoding = 'utf-8', index = False)\n",
    "        except:\n",
    "            print('Exception on file {}'.format(f))\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data for PCA usage, PCA Part goes to data modeling\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def fe_nm(input_sr):\n",
    "#     scl_result = np.empty()\n",
    "    \n",
    "    normalizer = preprocessing.Normalizer().fit(input_sr)\n",
    "    scl_result = normalizer.transform(input_sr)\n",
    "#     scl_result = preprocessing.normalize(input_sr, norm='l2') # commented to use normalizer\n",
    "    \n",
    "    return scl_result\n",
    "\n",
    "def df_rename(data, col):\n",
    "    l = len(col)\n",
    "    \n",
    "    for i in range(l):\n",
    "        colnew = col[i] + '_nm'\n",
    "        data_rename = data.rename(columns={i:colnew}, inplace=True)\n",
    "    \n",
    "    return data_rename\n",
    "\n",
    "def main():\n",
    "    processedList = []\n",
    "\n",
    "    loadPath = 'yourloadpath'\n",
    "    savePath = 'yoursavepath'\n",
    "\n",
    "    fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "#     fileList = ['002645.SZ.csv']\n",
    "\n",
    "    for f in fileList:\n",
    "        fileName = loadPath + f\n",
    "        data = pd.read_csv(fileName)\n",
    "        idxList = data.index.values.tolist()\n",
    "        \n",
    "        # fill inf and null\n",
    "        data = data.replace([np.inf, -np.inf], 0)\n",
    "        data.fillna(0, inplace=True)\n",
    "        \n",
    "        # save all column names that should be done normalization into the list below\n",
    "        colName = [  # columns for normalizing only\n",
    "                             'MTM_10_plr',\n",
    "                             'ROC_10_plr',\n",
    "                             'WNR_9',\n",
    "                             'WNR_9_maxmin',\n",
    "                             'SLOWK',\n",
    "                             'SLOWK_maxmin',\n",
    "                             'ROC_10',\n",
    "                             'SLOWD_flc',\n",
    "                             'WNR_9_flc',\n",
    "                             'RSI_5',\n",
    "                             'BIAS_20_maxmin',\n",
    "                             'RSI_5_maxmin',\n",
    "                             'BIAS_20',\n",
    "                             'SMA_10',\n",
    "                             'SLOWD',\n",
    "                             'SMA_10_maxmin',\n",
    "                             'CCI_24',\n",
    "                             'SLOWD_maxmin',\n",
    "                             'SMA_10_flc',\n",
    "                             'pct_chg',\n",
    "                             'MACD_HIST',\n",
    "                             'BR_26',\n",
    "                             'low',\n",
    "                             'change',\n",
    "                             'ADOSC',\n",
    "                             'SLOWK_flc',\n",
    "                             'RSI_5_flc',\n",
    "                             'MTM_10',\n",
    "                             'ADOSC_maxmin'\n",
    "                  ]\n",
    "        \n",
    "        try:\n",
    "            result = fe_nm(data[colName])\n",
    "\n",
    "            data_nm = pd.DataFrame.from_records(result)\n",
    "\n",
    "            data_aft = pd.concat([data, data_nm], axis=1)\n",
    "\n",
    "            df_rename(data_aft, colName)\n",
    "\n",
    "            colNameFull = [  # all the columns\n",
    "                             'ts_code',\n",
    "                             'trade_date', \n",
    "                             'MTM_10_plr_nm',\n",
    "                             'ROC_10_plr_nm',\n",
    "                             'WNR_9_nm',\n",
    "                             'WNR_9_maxmin_nm',\n",
    "                             'SLOWK_nm',\n",
    "                             'SLOWK_maxmin_nm',\n",
    "                             'ROC_10_nm',\n",
    "                             'SLOWD_flc_nm',\n",
    "                             'WNR_9_flc_nm',\n",
    "                             'RSI_5_nm',\n",
    "                             'BIAS_20_maxmin_nm',\n",
    "                             'RSI_5_maxmin_nm',\n",
    "                             'BIAS_20_nm',\n",
    "                             'SMA_10_nm',\n",
    "                             'SLOWD_nm',\n",
    "                             'SMA_10_maxmin_nm',\n",
    "                             'CCI_24_nm',\n",
    "                             'SLOWD_maxmin_nm',\n",
    "                             'SMA_10_flc_nm',\n",
    "                             'pct_chg_nm',\n",
    "                             'MACD_HIST_nm',\n",
    "                             'BR_26_nm',\n",
    "                             'low_nm',\n",
    "                             'change_nm',\n",
    "                             'ADOSC_nm',\n",
    "                             'SLOWK_flc_nm',\n",
    "                             'RSI_5_flc_nm',\n",
    "                             'MTM_10_nm',\n",
    "                             'ADOSC_maxmin_nm',\n",
    "                             'w_up_down'\n",
    "                          ]\n",
    "\n",
    "            data_aft = data_aft[colNameFull]\n",
    "\n",
    "            fileName = savePath + f\n",
    "            data_aft.to_csv(fileName, encoding = 'utf-8', index = False)\n",
    "        \n",
    "        except:\n",
    "            print('Exception on file {}'.format(f))\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feed 29 features without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, 29])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    y1 = data['w_up_down'].values\n",
    "    X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "    X1.fillna(0, inplace=True)\n",
    "    X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "#     try:\n",
    "    X1 = scaler.fit_transform(X1)\n",
    "    y0 = np.zeros([1,1])\n",
    "    y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "    y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "    X = np.vstack((X, X1))\n",
    "    y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "    X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "    y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "#     except:\n",
    "#         print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "# fileList = random.sample(fileList, 100)\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, 29])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=29, activation='relu'))\n",
    "model.add(Dense(29, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. PCA process to 20 principal components with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 20\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))  # commented, since the dataset is already normalized\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "#         X1 = scaler.fit_transform(X1)  # commented, since the dataset is already normalized\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "#         X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=num_pc, activation='relu'))\n",
    "model.add(Dense(num_pc, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. PCA process to 20 principal components without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 20\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "        X1 = scaler.fit_transform(X1)\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=num_pc, activation='relu'))\n",
    "model.add(Dense(num_pc, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. PCA process to 20 principal components without any preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 20\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "#         X1 = scaler.fit_transform(X1)\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "#         X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=num_pc, activation='relu'))\n",
    "model.add(Dense(num_pc, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PCA process to 15 principal components without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 15\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "        X1 = scaler.fit_transform(X1)\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=num_pc, activation='relu'))\n",
    "model.add(Dense(num_pc, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PCA process to 10 principal components without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 10\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "        X1 = scaler.fit_transform(X1)\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=num_pc, activation='relu'))\n",
    "model.add(Dense(num_pc, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PCA process to 5 principal components without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 5\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "        X1 = scaler.fit_transform(X1)\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=num_pc, activation='relu'))\n",
    "model.add(Dense(num_pc, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. PCA process to 5 principal components without normalization, test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 5\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "        X1 = scaler.fit_transform(X1)\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=num_pc, activation='relu'))\n",
    "model.add(Dense(num_pc, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=5, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
