{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 3 lengths result from step1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tempfile import TemporaryFile\n",
    "import random\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileName = ['RFE_2days_prediction.csv', 'RFE_5days_prediction.csv', 'RFE_10days_prediction.csv', ]\n",
    "\n",
    "fileName1 = loadPath + fileName[0]\n",
    "fileName2 = loadPath + fileName[1]\n",
    "fileName3 = loadPath + fileName[2]\n",
    "data_2 = pd.read_csv(fileName1)\n",
    "data_5 = pd.read_csv(fileName2)\n",
    "data_10 = pd.read_csv(fileName3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by relevant ranking\n",
    "data_2.sort_values('Rank', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for step 2, use the weekly labeled dataset, this cell is for expanded features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "    \n",
    "# We get the result from data exploring\n",
    "# Extract effective features from the original dataset and save them\n",
    "\n",
    "def main():\n",
    "    processedList = []\n",
    "\n",
    "    loadPath = 'yourloadpath'\n",
    "    savePath = 'yoursavepath'\n",
    "\n",
    "    fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "#     fileList = ['002645.SZ.csv']\n",
    "    \n",
    "    for f in fileList:\n",
    "        fileName = loadPath + f\n",
    "        data = pd.read_csv(fileName)\n",
    "        idxList = data.index.values.tolist()\n",
    "\n",
    "        # save all column names that should be done normalization into the list below\n",
    "        colName = [\n",
    "                     'ts_code',\n",
    "                     'trade_date', \n",
    "                     'SLOWK_maxmin',\n",
    "                     'SLOWK',\n",
    "                     'SLOWD_maxmin',\n",
    "                     'RSI_5_maxmin',\n",
    "                     'SLOWD',\n",
    "                     'RSI_5',\n",
    "                     'SLOWK_flc',\n",
    "                     'WNR_9_maxmin',\n",
    "                     'w_up_down'\n",
    "                  ]\n",
    "        try:\n",
    "            data = data[colName]\n",
    "            fileName = savePath + f\n",
    "            data.to_csv(fileName, encoding = 'utf-8', index = False)\n",
    "        except:\n",
    "            print('Exception on file {}'.format(f))\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for step 2, use the weekly labeled dataset, this cell is for original features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "    \n",
    "# We get the result from data exploring\n",
    "# Extract effective features from the original dataset and save them\n",
    "\n",
    "def main():\n",
    "    processedList = []\n",
    "\n",
    "    loadPath = 'yourloadpath'\n",
    "    savePath = 'yoursavepath'\n",
    "\n",
    "    fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "#     fileList = ['002645.SZ.csv']\n",
    "    \n",
    "    for f in fileList:\n",
    "        fileName = loadPath + f\n",
    "        data = pd.read_csv(fileName)\n",
    "        idxList = data.index.values.tolist()\n",
    "\n",
    "        # save all column names that should be done normalization into the list below\n",
    "        colName = [\n",
    "                     'ts_code',\n",
    "                     'trade_date', \n",
    "                     'SLOWK',\n",
    "                     'SLOWD',\n",
    "                     'RSI_5',\n",
    "                     'w_up_down'\n",
    "                  ]\n",
    "        try:\n",
    "            data = data[colName]\n",
    "            fileName = savePath + f\n",
    "            data.to_csv(fileName, encoding = 'utf-8', index = False)\n",
    "        except:\n",
    "            print('Exception on file {}'.format(f))\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the extended feature set using simple NN\n",
    "# Step 1, training dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "fileList = fileList[-1000:]\n",
    "\n",
    "fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, 8])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    y1 = data['w_up_down'].values\n",
    "    X1 = data.iloc[:,2:10].copy() # To avoid the case where changing X also changes data\n",
    "    X1.fillna(0, inplace=True)\n",
    "    try:\n",
    "        X1 = scaler.fit_transform(X1)\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the extended feature set using simple NN\n",
    "# Step 2, test dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(fileList, 100)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, 8])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:10].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=10, batch_size=50)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the original feature set using simple NN\n",
    "# Step 1, training dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "fileList = fileList[-1000:]\n",
    "\n",
    "fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, 3])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    y1 = data['w_up_down'].values\n",
    "    X1 = data.iloc[:,2:5].copy() # To avoid the case where changing X also changes data\n",
    "    X1.fillna(0, inplace=True)\n",
    "    try:\n",
    "        X1 = scaler.fit_transform(X1)\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the extended feature set using simple NN\n",
    "# Step 2, test dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(fileList, 100)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, 3])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:5].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=3, activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=10, batch_size=50)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
