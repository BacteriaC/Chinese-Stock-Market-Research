{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "fileList = fileList[:500]\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, 10])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    y1 = data['w_up_down'].values\n",
    "    X1 = data.iloc[:,2:17].copy() # To avoid the case where changing X also changes data\n",
    "    X1.fillna(0, inplace=True)\n",
    "    X1 = scaler.fit_transform(X1) # Choose to scale the data\n",
    "#     X = normalize(X) # Choose to normalize the data\n",
    "    \n",
    "    ipca = IncrementalPCA(n_components=10) # We have 10 features and shrink to 5 using PCA\n",
    "    ipca.fit(X1)\n",
    "    X1 = ipca.transform(X1)\n",
    "    \n",
    "    X = np.vstack((X, X1))\n",
    "    y = np.append(y, y1)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "fileList = fileList[-100:]\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, 10])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    y1_test = data['w_up_down'].values.tolist()\n",
    "    X1_test = data.iloc[:,2:17].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = scaler.fit_transform(X1_test)\n",
    "    \n",
    "    ipca = IncrementalPCA(n_components=10)\n",
    "    ipca.fit(X1_test)\n",
    "    X1_test = ipca.transform(X1_test)\n",
    "    \n",
    "    X_test = np.vstack((X_test, X1_test))\n",
    "    y_test = np.append(y_test, y1_test)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=10, batch_size=10)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 epoch 10 batch size： 63.64%， 500 training 100 testing\n",
    "# 50 epoch 10 batch size: 64.64%, 500 training 100 testing\n",
    "# 10 epoch 10 batch size: 65.10%, 1000 training 100 testing\n",
    "\n",
    "\n",
    "# test block\n",
    "import numpy as np\n",
    "a = np.random.rand(2,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, 10])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    y1 = data['w_up_down'].values\n",
    "    X1 = data.iloc[:,2:17].copy() # To avoid the case where changing X also changes data\n",
    "    X1.fillna(0, inplace=True)\n",
    "    X1 = scaler.fit_transform(X1) # Choose to scale the data\n",
    "#     X = normalize(X) # Choose to normalize the data\n",
    "    \n",
    "    ipca = IncrementalPCA(n_components=10) # We have 10 features and shrink to 5 using PCA\n",
    "    ipca.fit(X1)\n",
    "    X1 = ipca.transform(X1)\n",
    "    \n",
    "    y0 = np.zeros([1,1])\n",
    "    y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "    y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "    \n",
    "    X = np.vstack((X, X1))\n",
    "    y = np.append(y, y1)  # stack all labels\n",
    "    \n",
    "    X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "    y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "fileList = fileList[300:400]\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, 10])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:17].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "    ipca = IncrementalPCA(n_components=10)\n",
    "    try:\n",
    "        ipca.fit(X1_test)\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = ipca.transform(X1_test)\n",
    "    \n",
    "    y0_test = np.zeros([1,1])\n",
    "    y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "    y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "    \n",
    "    X_test = np.vstack((X_test, X1_test))\n",
    "    y_test = np.append(y_test, y1_test)\n",
    "    \n",
    "    X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "    y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=150, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
