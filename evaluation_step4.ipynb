{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR test on all 29 features, bi-weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, 29])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    y1 = data['w_up_down'].values\n",
    "    X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "    X1.fillna(0, inplace=True)\n",
    "    X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "#     try:\n",
    "    X1 = scaler.fit_transform(X1)\n",
    "    y0 = np.zeros([1,1])\n",
    "    y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "    y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "    X = np.vstack((X, X1))\n",
    "    y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "    X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "    y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "#     except:\n",
    "#         print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "# fileList = random.sample(fileList, 100)\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, 29])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling - Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifier using logistic regression\n",
    "# Easy to interpret\n",
    "# Linear models tend to perform well on sparse datasets like this one\n",
    "# Learn very fast compared to other algorithms.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X, y)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))\n",
    "\n",
    "# Hyperparameter C adjusts the regularization. We found C = 0.05 performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtarget = [0 if i < 500 else 1 for i in range(1000)]\n",
    "# final_model = LogisticRegression(C=0.5)\n",
    "# final_model.fit(X, target)\n",
    "# model = final_model.fit(X_train, y_train)\n",
    "# print (\"Final Accuracy: %s\" \n",
    "#        % accuracy_score(newtarget, final_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, f score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "score_spt_macro = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "score_spt_micro = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "score_spt_weighted = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "score_spt_none = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "\n",
    "print('Score support of precision recall and f1 score: macro: {}, micro: {}, weighted: {}, none: {}'.format(score_spt_macro, score_spt_micro, score_spt_weighted, score_spt_none))\n",
    "print('Macro precision is: {}, micro precision is: {}, weighted precision is: {}'.format(score_spt_macro[0], score_spt_micro[0], score_spt_weighted[0]))\n",
    "print('Macro recall is: {}, micro recall is: {}, weighted recall is: {}'.format(score_spt_macro[1], score_spt_micro[1], score_spt_weighted[1]))\n",
    "print('Macro f1 score is: {}, micro f1 score is: {}, weighted f1 score is: {}'.format(score_spt_macro[2], score_spt_micro[2], score_spt_weighted[2]))\n",
    "print('Score support of precision recall and f1 score: None: {}'.format(score_spt_none))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# hyper parameter test\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lsvm = LinearSVC(C=c)\n",
    "    lsvm.fit(X, y)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lsvm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lsvm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, f score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "score_spt_macro = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "score_spt_micro = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "score_spt_weighted = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "score_spt_none = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "\n",
    "print('Score support of precision recall and f1 score: macro: {}, micro: {}, weighted: {}, none: {}'.format(score_spt_macro, score_spt_micro, score_spt_weighted, score_spt_none))\n",
    "print('Macro precision is: {}, micro precision is: {}, weighted precision is: {}'.format(score_spt_macro[0], score_spt_micro[0], score_spt_weighted[0]))\n",
    "print('Macro recall is: {}, micro recall is: {}, weighted recall is: {}'.format(score_spt_macro[1], score_spt_micro[1], score_spt_weighted[1]))\n",
    "print('Macro f1 score is: {}, micro f1 score is: {}, weighted f1 score is: {}'.format(score_spt_macro[2], score_spt_micro[2], score_spt_weighted[2]))\n",
    "print('Score support of precision recall and f1 score: None: {}'.format(score_spt_none))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    bnbc = BernoulliNB(binarize=None, alpha=c)\n",
    "    bnbc.fit(X, y)\n",
    "    \n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, bnbc.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bnbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, f score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "score_spt_macro = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "score_spt_micro = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "score_spt_weighted = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "score_spt_none = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "\n",
    "print('Score support of precision recall and f1 score: macro: {}, micro: {}, weighted: {}, none: {}'.format(score_spt_macro, score_spt_micro, score_spt_weighted, score_spt_none))\n",
    "print('Macro precision is: {}, micro precision is: {}, weighted precision is: {}'.format(score_spt_macro[0], score_spt_micro[0], score_spt_weighted[0]))\n",
    "print('Macro recall is: {}, micro recall is: {}, weighted recall is: {}'.format(score_spt_macro[1], score_spt_micro[1], score_spt_weighted[1]))\n",
    "print('Macro f1 score is: {}, micro f1 score is: {}, weighted f1 score is: {}'.format(score_spt_macro[2], score_spt_micro[2], score_spt_weighted[2]))\n",
    "print('Score support of precision recall and f1 score: None: {}'.format(score_spt_none))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model a simple MLP NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define nn\n",
    "\n",
    "num_pc = 29\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim=num_pc, activation='relu'))\n",
    "model.add(Dense(num_pc, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, f score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "score_spt_macro = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "score_spt_micro = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "score_spt_weighted = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "score_spt_none = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "\n",
    "print('Score support of precision recall and f1 score: macro: {}, micro: {}, weighted: {}, none: {}'.format(score_spt_macro, score_spt_micro, score_spt_weighted, score_spt_none))\n",
    "print('Macro precision is: {}, micro precision is: {}, weighted precision is: {}'.format(score_spt_macro[0], score_spt_micro[0], score_spt_weighted[0]))\n",
    "print('Macro recall is: {}, micro recall is: {}, weighted recall is: {}'.format(score_spt_macro[1], score_spt_micro[1], score_spt_weighted[1]))\n",
    "print('Macro f1 score is: {}, micro f1 score is: {}, weighted f1 score is: {}'.format(score_spt_macro[2], score_spt_micro[2], score_spt_weighted[2]))\n",
    "print('Score support of precision recall and f1 score: None: {}'.format(score_spt_none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras CNN but not useful\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dropout\n",
    "\n",
    "TIME_PERIODS = 1\n",
    "num_pc = 29\n",
    "num_classes = 1\n",
    "input_shape = num_pc\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "model_m = Sequential()\n",
    "model_m.add(Reshape((TIME_PERIODS, num_pc), input_shape=(input_shape,)))\n",
    "model_m.add(Conv1D(100, 10, activation='relu', input_shape=(TIME_PERIODS, num_pc)))\n",
    "model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "model_m.add(MaxPooling1D(3))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(GlobalAveragePooling1D())\n",
    "model_m.add(Dropout(0.5))\n",
    "model_m.add(Dense(num_classes, activation='softmax'))\n",
    "print(model_m.summary())\n",
    "\n",
    "# define nn\n",
    "\n",
    "# create model\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=100)  # default 150 epochs\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - 29 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "# from sklearn import preprocessing\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import concatenate\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "N_FEATURES = 29\n",
    "N_TIME_STEPS = 10\n",
    "\n",
    "# datasetpre1 = X  # delete 'index_col=0' in bracket because of lack of index\n",
    "# datasetpre = pd.get_dummies(datasetpre1,drop_first=True)  # GET DUMMMIE and drop\n",
    "# X = datasetpre.values\n",
    "# # integer encode, IF OUT OF INDEX ERROR, CLOSE THE DISTRIBUTION AND RELAUNCH\n",
    "# # Comment label encoder because of naming error, only onehotencoder works fine now\n",
    "# label_encoder_0 = LabelEncoder()\n",
    "# #X[:,0] = label_encoder_0.fit_transform(values[:,0])\n",
    "# X[:,0] = label_encoder_0.fit_transform(datasetpre.values[:,0])\n",
    "# label_encoder_1 = LabelEncoder()\n",
    "# #X[:,1] = label_encoder_1.fit_transform(values[:,1])\n",
    "# X[:,1] = label_encoder_1.fit_transform(datasetpre.values[:,1])\n",
    "# label_encoder_2 = LabelEncoder()\n",
    "# #X[:,2] = label_encoder_2.fit_transform(values[:,2])\n",
    "# X[:,2] = label_encoder_2.fit_transform(datasetpre.values[:,2])\n",
    "# # print(integer_encoded)\n",
    "\n",
    "# # Onehotencoder will make the matrix really large, not fit second-based data structure\n",
    "# #onehotencoder = OneHotEncoder(categorical_features = \"all\")\n",
    "# #X = onehotencoder.fit_transform(X).toarray()\n",
    "# # ensure all data is float\n",
    "# X = X.astype('float32')\n",
    "# # normalize features\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaled = scaler.fit_transform(X)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(X, N_TIME_STEPS, 1)\n",
    "print(reframed.shape)\n",
    "\n",
    "# # binary encode\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "# print(onehot_encoded)\n",
    "# # invert first example\n",
    "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[1, :])])\n",
    "# print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X = reframed.values\n",
    "N_TRAIN_TIME = 591200\n",
    "train = X[:N_TRAIN_TIME, :]\n",
    "test = X[N_TRAIN_TIME:, :]\n",
    "# split into input and outputs\n",
    "N_OBS = N_TIME_STEPS * N_FEATURES\n",
    "train_X = train[:, :N_OBS]\n",
    "train_y = y_new[:N_TRAIN_TIME]\n",
    "test_X = test[:, :N_OBS]\n",
    "test_y = y_new[N_TRAIN_TIME:]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], N_TIME_STEPS, N_FEATURES))\n",
    "test_X = test_X.reshape((test_X.shape[0], N_TIME_STEPS, N_FEATURES))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "# Below is for F1 metrics\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "#         \"\"\"Recall metric.\n",
    "\n",
    "#         Only computes a batch-wise average of recall.\n",
    "\n",
    "#         Computes the recall, a metric for multi-label classification of\n",
    "#         how many relevant items are selected.\n",
    "#         \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "#         \"\"\"Precision metric.\n",
    "\n",
    "#         Only computes a batch-wise average of precision.\n",
    "\n",
    "#         Computes the precision, a metric for multi-label classification of\n",
    "#         how many selected items are relevant.\n",
    "#         \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "#model.compile(loss='mae', optimizer='adam')\n",
    "#model.compile(loss='mae', optimizer='adam', metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "model.compile(loss='mae',\n",
    "              optimizer= \"adam\",\n",
    "              metrics=[f1, metrics.binary_accuracy, metrics.mean_squared_error, metrics.mean_absolute_error])\n",
    "# fit network\n",
    "#history = model.fit(train_X, train_y, epochs=6, batch_size=1440, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=3000, \n",
    "                    validation_data=(test_X, test_y), shuffle=True)\n",
    "score = model.evaluate(test_X, test_y, batch_size=3000)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(test_X, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = model.evaluate(test_X, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[4], scores[4]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, f score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "score_spt_macro = precision_recall_fscore_support(test_y, y_pred, average='macro')\n",
    "score_spt_micro = precision_recall_fscore_support(test_y, y_pred, average='micro')\n",
    "score_spt_weighted = precision_recall_fscore_support(test_y, y_pred, average='weighted')\n",
    "score_spt_none = precision_recall_fscore_support(test_y, y_pred, average=None)\n",
    "\n",
    "print('Score support of precision recall and f1 score: macro: {}, micro: {}, weighted: {}, none: {}'.format(score_spt_macro, score_spt_micro, score_spt_weighted, score_spt_none))\n",
    "print('Macro precision is: {}, micro precision is: {}, weighted precision is: {}'.format(score_spt_macro[0], score_spt_micro[0], score_spt_weighted[0]))\n",
    "print('Macro recall is: {}, micro recall is: {}, weighted recall is: {}'.format(score_spt_macro[1], score_spt_micro[1], score_spt_weighted[1]))\n",
    "print('Macro f1 score is: {}, micro f1 score is: {}, weighted f1 score is: {}'.format(score_spt_macro[2], score_spt_micro[2], score_spt_weighted[2]))\n",
    "print('Score support of precision recall and f1 score: None: {}'.format(score_spt_none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - 5 Principal Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 5\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "        X1 = scaler.fit_transform(X1)\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "# from sklearn import preprocessing\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import concatenate\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "N_FEATURES = 5\n",
    "N_TIME_STEPS = 10\n",
    "\n",
    "# datasetpre1 = X  # delete 'index_col=0' in bracket because of lack of index\n",
    "# datasetpre = pd.get_dummies(datasetpre1,drop_first=True)  # GET DUMMMIE and drop\n",
    "# X = datasetpre.values\n",
    "# # integer encode, IF OUT OF INDEX ERROR, CLOSE THE DISTRIBUTION AND RELAUNCH\n",
    "# # Comment label encoder because of naming error, only onehotencoder works fine now\n",
    "# label_encoder_0 = LabelEncoder()\n",
    "# #X[:,0] = label_encoder_0.fit_transform(values[:,0])\n",
    "# X[:,0] = label_encoder_0.fit_transform(datasetpre.values[:,0])\n",
    "# label_encoder_1 = LabelEncoder()\n",
    "# #X[:,1] = label_encoder_1.fit_transform(values[:,1])\n",
    "# X[:,1] = label_encoder_1.fit_transform(datasetpre.values[:,1])\n",
    "# label_encoder_2 = LabelEncoder()\n",
    "# #X[:,2] = label_encoder_2.fit_transform(values[:,2])\n",
    "# X[:,2] = label_encoder_2.fit_transform(datasetpre.values[:,2])\n",
    "# # print(integer_encoded)\n",
    "\n",
    "# # Onehotencoder will make the matrix really large, not fit second-based data structure\n",
    "# #onehotencoder = OneHotEncoder(categorical_features = \"all\")\n",
    "# #X = onehotencoder.fit_transform(X).toarray()\n",
    "# # ensure all data is float\n",
    "# X = X.astype('float32')\n",
    "# # normalize features\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaled = scaler.fit_transform(X)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(X, N_TIME_STEPS, 1)\n",
    "print(reframed.shape)\n",
    "\n",
    "# # binary encode\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "# print(onehot_encoded)\n",
    "# # invert first example\n",
    "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[1, :])])\n",
    "# print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X = reframed.values\n",
    "N_TRAIN_TIME = 591200\n",
    "train = X[:N_TRAIN_TIME, :]\n",
    "test = X[N_TRAIN_TIME:, :]\n",
    "# split into input and outputs\n",
    "N_OBS = N_TIME_STEPS * N_FEATURES\n",
    "train_X = train[:, :N_OBS]\n",
    "train_y = y_new[:N_TRAIN_TIME]\n",
    "test_X = test[:, :N_OBS]\n",
    "test_y = y_new[N_TRAIN_TIME:]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], N_TIME_STEPS, N_FEATURES))\n",
    "test_X = test_X.reshape((test_X.shape[0], N_TIME_STEPS, N_FEATURES))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "# Below is for F1 metrics\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "#         \"\"\"Recall metric.\n",
    "\n",
    "#         Only computes a batch-wise average of recall.\n",
    "\n",
    "#         Computes the recall, a metric for multi-label classification of\n",
    "#         how many relevant items are selected.\n",
    "#         \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "#         \"\"\"Precision metric.\n",
    "\n",
    "#         Only computes a batch-wise average of precision.\n",
    "\n",
    "#         Computes the precision, a metric for multi-label classification of\n",
    "#         how many selected items are relevant.\n",
    "#         \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "#model.compile(loss='mae', optimizer='adam')\n",
    "#model.compile(loss='mae', optimizer='adam', metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "model.compile(loss='mae',\n",
    "              optimizer= \"adam\",\n",
    "              metrics=[f1, metrics.binary_accuracy, metrics.mean_squared_error, metrics.mean_absolute_error])\n",
    "# fit network\n",
    "#history = model.fit(train_X, train_y, epochs=6, batch_size=1440, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=3000, \n",
    "                    validation_data=(test_X, test_y), shuffle=True)\n",
    "score = model.evaluate(test_X, test_y, batch_size=3000)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(test_X, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_X, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[4], scores[4]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, f score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "score_spt_macro = precision_recall_fscore_support(test_y, y_pred, average='macro')\n",
    "score_spt_micro = precision_recall_fscore_support(test_y, y_pred, average='micro')\n",
    "score_spt_weighted = precision_recall_fscore_support(test_y, y_pred, average='weighted')\n",
    "score_spt_none = precision_recall_fscore_support(test_y, y_pred, average=None)\n",
    "\n",
    "print('Score support of precision recall and f1 score: macro: {}, micro: {}, weighted: {}, none: {}'.format(score_spt_macro, score_spt_micro, score_spt_weighted, score_spt_none))\n",
    "print('Macro precision is: {}, micro precision is: {}, weighted precision is: {}'.format(score_spt_macro[0], score_spt_micro[0], score_spt_weighted[0]))\n",
    "print('Macro recall is: {}, micro recall is: {}, weighted recall is: {}'.format(score_spt_macro[1], score_spt_micro[1], score_spt_weighted[1]))\n",
    "print('Macro f1 score is: {}, micro f1 score is: {}, weighted f1 score is: {}'.format(score_spt_macro[2], score_spt_micro[2], score_spt_weighted[2]))\n",
    "print('Score support of precision recall and f1 score: None: {}'.format(score_spt_none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - 29 Features for Biweekly Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print (\"Accuracy is %s\" \n",
    "        % (accuracy_score(y_test, clf.predict(X_test))))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - RAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "classes_label = ['Up','Down']\n",
    " \n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes_label, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, f score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "score_spt_macro = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "score_spt_micro = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "score_spt_weighted = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "score_spt_none = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "\n",
    "print('Score support of precision recall and f1 score: macro: {}, micro: {}, weighted: {}, none: {}'.format(score_spt_macro, score_spt_micro, score_spt_weighted, score_spt_none))\n",
    "print('Macro precision is: {}, micro precision is: {}, weighted precision is: {}'.format(score_spt_macro[0], score_spt_micro[0], score_spt_weighted[0]))\n",
    "print('Macro recall is: {}, micro recall is: {}, weighted recall is: {}'.format(score_spt_macro[1], score_spt_micro[1], score_spt_weighted[1]))\n",
    "print('Macro f1 score is: {}, micro f1 score is: {}, weighted f1 score is: {}'.format(score_spt_macro[2], score_spt_micro[2], score_spt_weighted[2]))\n",
    "print('Score support of precision recall and f1 score: None: {}'.format(score_spt_none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - 5 Principal Component - for learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test, move the ground truth one day after, use the data from previous day to predict later day\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "num_pc = 5\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "\n",
    "# fileList = fileList[-1000:]\n",
    "\n",
    "# fileList = random.sample(fileList, 500)\n",
    "\n",
    "# fileList = ['002645.SZ.csv', '603876.SH.csv', '002638.SZ.csv']\n",
    "\n",
    "selectedList = random.sample(fileList, 2000)\n",
    "restList = list(set(fileList) - set(selectedList))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = np.zeros([1, num_pc])\n",
    "y = np.zeros([1, 1])\n",
    "\n",
    "for f in selectedList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1 = data['w_up_down'].values\n",
    "        X1 = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data\n",
    "        X1.fillna(0, inplace=True)\n",
    "        X1 = X1.replace(np.inf, 0)  # Replace inf with 0\n",
    "    \n",
    "        X1 = scaler.fit_transform(X1)\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1)\n",
    "        X1 = ipca.transform(X1)\n",
    "\n",
    "        y0 = np.zeros([1,1])\n",
    "        y1 = np.append(y1, y0)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1 = np.delete(y1, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X = np.vstack((X, X1))\n",
    "        y = np.append(y, y1)  # stack all labels\n",
    "\n",
    "        X = X[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y = y[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X = np.delete(X, (0), axis=0) # delete first row\n",
    "y = np.delete(y, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test set of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import sleep\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "# count the time, start point\n",
    "start = time.time()\n",
    "\n",
    "processedList = []\n",
    "\n",
    "loadPath = 'yourloadpath'\n",
    "savePath = 'yoursavepath'\n",
    "\n",
    "fileList = [f for f in listdir(loadPath) if isfile(join(loadPath, f))]\n",
    "# fileList = fileList[:2000]\n",
    "\n",
    "fileList = random.sample(restList, 500)\n",
    "\n",
    "# fileList = ['002169.SZ.csv']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_test = np.zeros([1, num_pc])\n",
    "y_test = np.zeros([1, 1])\n",
    "\n",
    "for f in fileList:\n",
    "    fileName = loadPath + f\n",
    "    data = pd.read_csv(fileName)\n",
    "    \n",
    "    try:\n",
    "        y1_test = data['w_up_down'].values.tolist()\n",
    "    except:\n",
    "        print(f)\n",
    "    X1_test = data.iloc[:,2:31].copy() # To avoid the case where changing X also changes data, 2 to 27\n",
    "    X1_test.fillna(0, inplace=True)\n",
    "    X1_test = X1_test.replace(np.inf, 0)  # Replace inf with 0\n",
    "    try:\n",
    "        X1_test = scaler.fit_transform(X1_test)\n",
    "        \n",
    "        ipca = IncrementalPCA(n_components=num_pc)\n",
    "        ipca.fit(X1_test)\n",
    "        X1_test = ipca.transform(X1_test)\n",
    "        \n",
    "        y0_test = np.zeros([1,1])\n",
    "        y1_test = np.append(y1_test, y0_test)  # add a 0 at the end of label and pop all label up one day, use today data to predict tomorrow result\n",
    "        y1_test = np.delete(y1_test, (0), axis=0) # delete first row to ensure the lenth is correct\n",
    "\n",
    "        X_test = np.vstack((X_test, X1_test))\n",
    "        y_test = np.append(y_test, y1_test)\n",
    "\n",
    "        X_test = X_test[:-1, :]  # delete the last row of X because the last row of indices are correspoinding to y0\n",
    "        y_test = y_test[:-1]  # delete the last row of y because the last row of label is y0\n",
    "    except:\n",
    "        print(f)\n",
    "    \n",
    "X_test = np.delete(X_test, (0), axis=0) # delete first row\n",
    "y_test = np.delete(y_test, (0), axis=0) # delete first row\n",
    "\n",
    "# count the time, end point\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "# from sklearn import preprocessing\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import concatenate\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "N_FEATURES = 5\n",
    "N_TIME_STEPS = 10\n",
    "\n",
    "# datasetpre1 = X  # delete 'index_col=0' in bracket because of lack of index\n",
    "# datasetpre = pd.get_dummies(datasetpre1,drop_first=True)  # GET DUMMMIE and drop\n",
    "# X = datasetpre.values\n",
    "# # integer encode, IF OUT OF INDEX ERROR, CLOSE THE DISTRIBUTION AND RELAUNCH\n",
    "# # Comment label encoder because of naming error, only onehotencoder works fine now\n",
    "# label_encoder_0 = LabelEncoder()\n",
    "# #X[:,0] = label_encoder_0.fit_transform(values[:,0])\n",
    "# X[:,0] = label_encoder_0.fit_transform(datasetpre.values[:,0])\n",
    "# label_encoder_1 = LabelEncoder()\n",
    "# #X[:,1] = label_encoder_1.fit_transform(values[:,1])\n",
    "# X[:,1] = label_encoder_1.fit_transform(datasetpre.values[:,1])\n",
    "# label_encoder_2 = LabelEncoder()\n",
    "# #X[:,2] = label_encoder_2.fit_transform(values[:,2])\n",
    "# X[:,2] = label_encoder_2.fit_transform(datasetpre.values[:,2])\n",
    "# # print(integer_encoded)\n",
    "\n",
    "# # Onehotencoder will make the matrix really large, not fit second-based data structure\n",
    "# #onehotencoder = OneHotEncoder(categorical_features = \"all\")\n",
    "# #X = onehotencoder.fit_transform(X).toarray()\n",
    "# # ensure all data is float\n",
    "# X = X.astype('float32')\n",
    "# # normalize features\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaled = scaler.fit_transform(X)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(X, N_TIME_STEPS, 1)\n",
    "print(reframed.shape)\n",
    "\n",
    "# # binary encode\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "# print(onehot_encoded)\n",
    "# # invert first example\n",
    "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[1, :])])\n",
    "# print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X = reframed.values\n",
    "N_TRAIN_TIME = 591200\n",
    "train = X[:N_TRAIN_TIME, :]\n",
    "test = X[N_TRAIN_TIME:, :]\n",
    "# split into input and outputs\n",
    "N_OBS = N_TIME_STEPS * N_FEATURES\n",
    "train_X = train[:, :N_OBS]\n",
    "train_y = y_new[:N_TRAIN_TIME]\n",
    "test_X = test[:, :N_OBS]\n",
    "test_y = y_new[N_TRAIN_TIME:]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], N_TIME_STEPS, N_FEATURES))\n",
    "test_X = test_X.reshape((test_X.shape[0], N_TIME_STEPS, N_FEATURES))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "# Below is for F1 metrics\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "#         \"\"\"Recall metric.\n",
    "\n",
    "#         Only computes a batch-wise average of recall.\n",
    "\n",
    "#         Computes the recall, a metric for multi-label classification of\n",
    "#         how many relevant items are selected.\n",
    "#         \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "#         \"\"\"Precision metric.\n",
    "\n",
    "#         Only computes a batch-wise average of precision.\n",
    "\n",
    "#         Computes the precision, a metric for multi-label classification of\n",
    "#         how many selected items are relevant.\n",
    "#         \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "#model.compile(loss='mae', optimizer='adam')\n",
    "#model.compile(loss='mae', optimizer='adam', metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "model.compile(loss='mae',\n",
    "              optimizer= \"adam\",\n",
    "              metrics=[f1, metrics.binary_accuracy, metrics.mean_squared_error, metrics.mean_absolute_error])\n",
    "# fit network\n",
    "#history = model.fit(train_X, train_y, epochs=6, batch_size=1440, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "history = model.fit(train_X, train_y, epochs=150, batch_size=3000, \n",
    "                    validation_data=(test_X, test_y), shuffle=True)\n",
    "score = model.evaluate(test_X, test_y, batch_size=3000)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_elapse = end - start\n",
    "print(time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
